{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m      6\u001b[0m data_transforms \u001b[39m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m      8\u001b[0m         transforms\u001b[39m.\u001b[39mRandomResizedCrop(\u001b[39m224\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     ]),\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     22\u001b[0m \u001b[39m# Load the training and test datasets\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m image_datasets \u001b[39m=\u001b[39m {x: datasets\u001b[39m.\u001b[39mFlowers102(root\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./data/\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m                                          split\u001b[39m=\u001b[39mx,\n\u001b[0;32m     25\u001b[0m                                          transform\u001b[39m=\u001b[39mdata_transforms[x],\n\u001b[0;32m     26\u001b[0m                                          download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m                   \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m     28\u001b[0m \u001b[39m# Create dataloaders for training and test datasets\u001b[39;00m\n\u001b[0;32m     29\u001b[0m dataloaders \u001b[39m=\u001b[39m {x: DataLoader(image_datasets[x], batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m,\n\u001b[0;32m     30\u001b[0m                              shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m     31\u001b[0m               \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m]}\n",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m data_transforms \u001b[39m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: transforms\u001b[39m.\u001b[39mCompose([\n\u001b[0;32m      8\u001b[0m         transforms\u001b[39m.\u001b[39mRandomResizedCrop(\u001b[39m224\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     ]),\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     22\u001b[0m \u001b[39m# Load the training and test datasets\u001b[39;00m\n\u001b[0;32m     23\u001b[0m image_datasets \u001b[39m=\u001b[39m {x: datasets\u001b[39m.\u001b[39mFlowers102(root\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./data/\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m                                          split\u001b[39m=\u001b[39mx,\n\u001b[1;32m---> 25\u001b[0m                                          transform\u001b[39m=\u001b[39mdata_transforms[x],\n\u001b[0;32m     26\u001b[0m                                          download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     27\u001b[0m                   \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m     28\u001b[0m \u001b[39m# Create dataloaders for training and test datasets\u001b[39;00m\n\u001b[0;32m     29\u001b[0m dataloaders \u001b[39m=\u001b[39m {x: DataLoader(image_datasets[x], batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m,\n\u001b[0;32m     30\u001b[0m                              shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m     31\u001b[0m               \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mval\u001b[39m\u001b[39m\"\u001b[39m]}\n",
      "\u001b[1;31mKeyError\u001b[0m: 'val'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Define the transforms for the training and test datasets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "# Load the training and test datasets\n",
    "image_datasets = {x: datasets.Flowers102(root=\"./data/\",\n",
    "                                         split=x,\n",
    "                                         transform=data_transforms[x],\n",
    "                                         download=True)\n",
    "                  for x in ['train', 'test', \"val\"]}\n",
    "# Create dataloaders for training and test datasets\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=64,\n",
    "                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'test', 'val']}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
